{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from utils.imputation import ImputationPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078b4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/imputed/train_imputed.csv')\n",
    "test_df = pd.read_csv('dataset/imputed/test_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c71e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19542 entries, 0 to 19541\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  19542 non-null  int64  \n",
      " 1   temperature         19542 non-null  float64\n",
      " 2   irradiance          19542 non-null  float64\n",
      " 3   humidity            19542 non-null  float64\n",
      " 4   panel_age           19542 non-null  float64\n",
      " 5   maintenance_count   19542 non-null  float64\n",
      " 6   soiling_ratio       19542 non-null  float64\n",
      " 7   voltage             19542 non-null  float64\n",
      " 8   current             19542 non-null  float64\n",
      " 9   module_temperature  19542 non-null  float64\n",
      " 10  cloud_coverage      19542 non-null  float64\n",
      " 11  wind_speed          19542 non-null  float64\n",
      " 12  pressure            19542 non-null  float64\n",
      " 13  string_id           19542 non-null  object \n",
      " 14  error_code          19542 non-null  object \n",
      " 15  installation_type   19542 non-null  object \n",
      " 16  efficiency          19542 non-null  float64\n",
      "dtypes: float64(13), int64(1), object(3)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Solar Panel Performance Model Selection Pipeline...\n",
      "============================================================\n",
      "Loading raw data...\n",
      "Raw dataset shape: (20000, 17)\n",
      "Missing values in raw data:\n",
      "temperature           1001\n",
      "irradiance             987\n",
      "panel_age             1011\n",
      "maintenance_count     1027\n",
      "soiling_ratio         1010\n",
      "voltage                993\n",
      "current                977\n",
      "module_temperature     978\n",
      "cloud_coverage        1010\n",
      "error_code            5912\n",
      "installation_type     5028\n",
      "dtype: int64\n",
      "\n",
      "Step 1: Fixing data types...\n",
      "\n",
      "=== FIXING DATA TYPES FOR TRAINING DATA ===\n",
      "\n",
      "Processing humidity:\n",
      "Original dtype: object\n",
      "Non-numeric values found in humidity:\n",
      "humidity\n",
      "unknown    50\n",
      "error      40\n",
      "badval     37\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 127\n",
      "Valid numeric values: 19873\n",
      "Min: 0.011\n",
      "Max: 99.995\n",
      "Mean: 50.066\n",
      "\n",
      "Processing wind_speed:\n",
      "Original dtype: object\n",
      "Non-numeric values found in wind_speed:\n",
      "wind_speed\n",
      "badval     42\n",
      "error      41\n",
      "unknown    36\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 119\n",
      "Valid numeric values: 19881\n",
      "Min: 0.001\n",
      "Max: 14.999\n",
      "Mean: 7.413\n",
      "\n",
      "Processing pressure:\n",
      "Original dtype: object\n",
      "Non-numeric values found in pressure:\n",
      "pressure\n",
      "unknown    46\n",
      "error      45\n",
      "badval     44\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 135\n",
      "Valid numeric values: 19865\n",
      "Min: 970.087\n",
      "Max: 1052.866\n",
      "Mean: 1012.981\n",
      "\n",
      "=== DATA TYPE VERIFICATION ===\n",
      "Data types after fixing:\n",
      "humidity: float64\n",
      "wind_speed: float64\n",
      "pressure: float64\n",
      "\n",
      "Step 2: Applying imputation pipeline...\n",
      "Dataset shape after imputation: (20000, 17)\n",
      "Remaining missing values after imputation: 0\n",
      "Categorical columns: ['string_id', 'error_code', 'installation_type']\n",
      "Numerical columns: ['id', 'temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure']\n",
      "Creating preprocessing pipeline...\n",
      "Preparing train-test split...\n",
      "Training set shape: (16000, 16)\n",
      "Test set shape: (4000, 16)\n",
      "Defining models...\n",
      "Evaluating base models...\n",
      "Training Linear Regression...\n",
      "Training Ridge...\n",
      "Training Lasso...\n",
      "Training ElasticNet...\n",
      "Training Decision Tree...\n",
      "Training Random Forest...\n",
      "Training Extra Trees...\n",
      "Training Gradient Boosting...\n",
      "Training XGBoost...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training KNN...\n",
      "Training SVR...\n",
      "Performing hyperparameter tuning for top 5 models...\n",
      "Tuning Gradient Boosting...\n",
      "Tuning LightGBM...\n",
      "Selecting best model...\n",
      "Best Model: LightGBM_Tuned\n",
      "Best Test Custom Score: 89.3986\n",
      "\n",
      "================================================================================\n",
      "MODEL SELECTION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "BASE MODELS PERFORMANCE (on original scale):\n",
      "--------------------------------------------------\n",
      "                  Test_Custom_Score Test_RMSE   Test_R2 CV_RMSE_transformed\n",
      "Gradient Boosting         89.368757  0.106312    0.4378            0.616481\n",
      "LightGBM                  89.352156  0.106478  0.436043            0.622003\n",
      "CatBoost                  89.352115  0.106479  0.436039            0.621947\n",
      "Extra Trees               89.276314  0.107237  0.427981            0.629889\n",
      "SVR                       89.248691  0.107513   0.42503            0.621729\n",
      "Random Forest             89.216008   0.10784  0.421529            0.634573\n",
      "XGBoost                   89.046867  0.109531  0.403241            0.658133\n",
      "Ridge                     88.978654  0.110213  0.395785            0.645619\n",
      "Linear Regression         88.978503  0.110215  0.395768            0.645619\n",
      "KNN                       87.960451  0.120395  0.278987            0.760306\n",
      "Lasso                     85.708352  0.142916 -0.015985            1.000366\n",
      "ElasticNet                85.708352  0.142916 -0.015985            1.000366\n",
      "Decision Tree             84.420949  0.155791  -0.20727            0.917509\n",
      "\n",
      "TUNED MODELS PERFORMANCE (on original scale):\n",
      "--------------------------------------------------\n",
      "                        Test_Custom_Score Test_RMSE   Test_R2 CV_RMSE_transformed\n",
      "LightGBM_Tuned                  89.398559  0.106014  0.440948            0.614966\n",
      "Gradient Boosting_Tuned         89.393164  0.106068  0.440379            0.615566\n",
      "\n",
      "BEST MODEL: LightGBM_Tuned\n",
      "BEST SCORE: 89.3986\n",
      "\n",
      "Note: All metrics except CV_RMSE_transformed are calculated on original scale\n",
      "Best model with complete pipeline saved to best_solar_model.pkl\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Best model: LightGBM_Tuned with score: 89.3986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from utils.imputation import ImputationPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SolarPanelModelSelector:\n",
    "    def __init__(self, data_path='dataset/train.csv', test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the model selector with data loading and basic setup\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.preprocessor = None\n",
    "        self.imputer = None  # Add imputer attribute\n",
    "        \n",
    "    def fix_data_types(self, df, dataset_name):\n",
    "        \"\"\"Fix data type inconsistencies for specific columns\"\"\"\n",
    "        df_fixed = df.copy()\n",
    "        \n",
    "        # Define columns that should be numeric\n",
    "        numeric_columns_to_fix = ['humidity', 'wind_speed', 'pressure']\n",
    "        \n",
    "        print(f\"\\n=== FIXING DATA TYPES FOR {dataset_name} ===\")\n",
    "        \n",
    "        for col in numeric_columns_to_fix:\n",
    "            if col in df_fixed.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                print(f\"Original dtype: {df_fixed[col].dtype}\")\n",
    "                \n",
    "                # Check for non-numeric values before conversion\n",
    "                if df_fixed[col].dtype == 'object':\n",
    "                    # Display unique non-numeric values\n",
    "                    try:\n",
    "                        # Try to convert to numeric and see what fails\n",
    "                        numeric_conversion = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        non_numeric_mask = pd.isna(numeric_conversion) & df_fixed[col].notna()\n",
    "                        \n",
    "                        if non_numeric_mask.any():\n",
    "                            print(f\"Non-numeric values found in {col}:\")\n",
    "                            non_numeric_values = df_fixed.loc[non_numeric_mask, col].value_counts()\n",
    "                            print(non_numeric_values.head(10))\n",
    "                            \n",
    "                            # Handle common non-numeric patterns\n",
    "                            df_fixed[col] = df_fixed[col].astype(str)\n",
    "                            \n",
    "                            # Remove common problematic characters\n",
    "                            df_fixed[col] = df_fixed[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "                            df_fixed[col] = df_fixed[col].str.strip()\n",
    "                            \n",
    "                            # Handle empty strings\n",
    "                            df_fixed[col] = df_fixed[col].replace('', np.nan)\n",
    "                            df_fixed[col] = df_fixed[col].replace('nan', np.nan)\n",
    "                            \n",
    "                        # Convert to numeric\n",
    "                        df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        \n",
    "                        print(f\"Converted dtype: {df_fixed[col].dtype}\")\n",
    "                        print(f\"Missing values after conversion: {df_fixed[col].isnull().sum()}\")\n",
    "                        print(f\"Valid numeric values: {df_fixed[col].notna().sum()}\")\n",
    "                        \n",
    "                        # Basic statistics for converted column\n",
    "                        if df_fixed[col].notna().any():\n",
    "                            print(f\"Min: {df_fixed[col].min():.3f}\")\n",
    "                            print(f\"Max: {df_fixed[col].max():.3f}\")\n",
    "                            print(f\"Mean: {df_fixed[col].mean():.3f}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting {col}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"{col} is already numeric type: {df_fixed[col].dtype}\")\n",
    "        \n",
    "        return df_fixed\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data, fix data types, and apply imputation pipeline\n",
    "        \"\"\"\n",
    "        print(\"Loading raw data...\")\n",
    "        self.df_raw = pd.read_csv(self.data_path)\n",
    "        print(f\"Raw dataset shape: {self.df_raw.shape}\")\n",
    "        print(f\"Missing values in raw data:\\n{self.df_raw.isnull().sum()[self.df_raw.isnull().sum() > 0]}\")\n",
    "        \n",
    "        # Step 1: Fix data types BEFORE imputation\n",
    "        print(\"\\nStep 1: Fixing data types...\")\n",
    "        self.df_fixed = self.fix_data_types(self.df_raw, \"TRAINING DATA\")\n",
    "        \n",
    "        # Verify the fixes\n",
    "        print(\"\\n=== DATA TYPE VERIFICATION ===\")\n",
    "        print(\"Data types after fixing:\")\n",
    "        for col in ['humidity', 'wind_speed', 'pressure']:\n",
    "            if col in self.df_fixed.columns:\n",
    "                print(f\"{col}: {self.df_fixed[col].dtype}\")\n",
    "        \n",
    "        # Step 2: Initialize and apply imputation pipeline\n",
    "        print(\"\\nStep 2: Applying imputation pipeline...\")\n",
    "        self.imputer = ImputationPipeline()\n",
    "        self.df = self.imputer.fit_transform(self.df_fixed)\n",
    "        \n",
    "        print(f\"Dataset shape after imputation: {self.df.shape}\")\n",
    "        remaining_missing = self.df.isnull().sum().sum()\n",
    "        print(f\"Remaining missing values after imputation: {remaining_missing}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        self.target_col = 'efficiency'\n",
    "        self.feature_cols = [col for col in self.df.columns if col != self.target_col]\n",
    "        \n",
    "        # Identify categorical and numerical columns\n",
    "        self.categorical_cols = self.df[self.feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "        self.numerical_cols = self.df[self.feature_cols].select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        \n",
    "        print(f\"Categorical columns: {self.categorical_cols}\")\n",
    "        print(f\"Numerical columns: {self.numerical_cols}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for numerical and categorical features\n",
    "        \"\"\"\n",
    "        print(\"Creating preprocessing pipeline...\")\n",
    "        \n",
    "        # Numerical preprocessing pipeline\n",
    "        numerical_pipeline = Pipeline([\n",
    "            ('scaler', RobustScaler())  # RobustScaler is less sensitive to outliers\n",
    "        ])\n",
    "        \n",
    "        # Categorical preprocessing pipeline\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('encoder', 'passthrough')  # Will be handled separately\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('num', numerical_pipeline, self.numerical_cols),\n",
    "            ('cat', categorical_pipeline, self.categorical_cols)\n",
    "        ])\n",
    "        \n",
    "        return self.preprocessor\n",
    "    \n",
    "    def prepare_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Prepare train-test split with proper preprocessing\n",
    "        \"\"\"\n",
    "        print(\"Preparing train-test split...\")\n",
    "        \n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        y = self.df[self.target_col].copy()\n",
    "        \n",
    "        # Store original target values for later use\n",
    "        self.y_original = y.copy()\n",
    "        \n",
    "        # Apply power transformation to target if it's skewed\n",
    "        self.target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "        y_transformed = self.target_transformer.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_transformed, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Also split original target for evaluation\n",
    "        _, _, self.y_train_original, self.y_test_original = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Handle categorical encoding\n",
    "        self.label_encoders = {}\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = le.transform(X_test[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        \n",
    "        # Apply numerical preprocessing\n",
    "        X_train_scaled = self.preprocessor.fit_transform(X_train)\n",
    "        X_test_scaled = self.preprocessor.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train_scaled, X_test_scaled\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        \n",
    "        print(f\"Training set shape: {self.X_train.shape}\")\n",
    "        print(f\"Test set shape: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def define_models(self):\n",
    "        \"\"\"\n",
    "        Define all models to be tested\n",
    "        \"\"\"\n",
    "        print(\"Defining models...\")\n",
    "        \n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(random_state=self.random_state),\n",
    "            'Lasso': Lasso(random_state=self.random_state),\n",
    "            'ElasticNet': ElasticNet(random_state=self.random_state),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=self.random_state),\n",
    "            'Random Forest': RandomForestRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Extra Trees': ExtraTreesRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=self.random_state),\n",
    "            'XGBoost': XGBRegressor(random_state=self.random_state, eval_metric='rmse'),\n",
    "            'LightGBM': LGBMRegressor(random_state=self.random_state, verbose=-1),\n",
    "            'CatBoost': CatBoostRegressor(random_state=self.random_state, verbose=False),\n",
    "            'KNN': KNeighborsRegressor(),\n",
    "            'SVR': SVR()\n",
    "        }\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_pred):\n",
    "        \"\"\"\n",
    "        Apply inverse transformation to predictions to get them back to original scale\n",
    "        \"\"\"\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_pred_original = self.target_transformer.inverse_transform(y_pred_reshaped).flatten()\n",
    "        return y_pred_original\n",
    "    \n",
    "    def custom_score_function(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Custom scoring function as per problem statement\n",
    "        Score = 100*(1-sqrt(MSE))\n",
    "        Note: This should be calculated on original scale, not transformed scale\n",
    "        \"\"\"\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        score = 100 * (1 - np.sqrt(mse))\n",
    "        return score\n",
    "    \n",
    "    def evaluate_base_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all base models using cross-validation\n",
    "        \"\"\"\n",
    "        print(\"Evaluating base models...\")\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            # Cross-validation scores (on transformed target)\n",
    "            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, \n",
    "                                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            \n",
    "            # Fit model for additional metrics\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Get predictions on transformed scale\n",
    "            y_pred_train_transformed = model.predict(self.X_train)\n",
    "            y_pred_test_transformed = model.predict(self.X_test)\n",
    "            \n",
    "            # Transform predictions back to original scale\n",
    "            y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "            y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "            \n",
    "            # Calculate metrics on ORIGINAL scale\n",
    "            train_rmse = np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original))\n",
    "            test_rmse = np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original))\n",
    "            train_r2 = r2_score(self.y_train_original, y_pred_train_original)\n",
    "            test_r2 = r2_score(self.y_test_original, y_pred_test_original)\n",
    "            \n",
    "            # Custom score on original scale\n",
    "            train_custom_score = self.custom_score_function(self.y_train_original, y_pred_train_original)\n",
    "            test_custom_score = self.custom_score_function(self.y_test_original, y_pred_test_original)\n",
    "            \n",
    "            # CV RMSE on transformed scale (for comparison)\n",
    "            cv_rmse_transformed = np.sqrt(-cv_scores.mean())\n",
    "            \n",
    "            self.results[name] = {\n",
    "                'CV_RMSE_transformed': cv_rmse_transformed,\n",
    "                'CV_RMSE_std': np.sqrt(cv_scores.std()),\n",
    "                'Train_RMSE': train_rmse,\n",
    "                'Test_RMSE': test_rmse,\n",
    "                'Train_R2': train_r2,\n",
    "                'Test_R2': test_r2,\n",
    "                'Train_Custom_Score': train_custom_score,\n",
    "                'Test_Custom_Score': test_custom_score,\n",
    "                'Model': model\n",
    "            }\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_hyperparameter_grids(self):\n",
    "        \"\"\"\n",
    "        Define hyperparameter grids for top performing models\n",
    "        \"\"\"\n",
    "        param_grids = {\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return param_grids\n",
    "    \n",
    "    def hyperparameter_tuning(self, top_n=5):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning for top N models\n",
    "        \"\"\"\n",
    "        print(f\"Performing hyperparameter tuning for top {top_n} models...\")\n",
    "        \n",
    "        # Sort models by test RMSE\n",
    "        sorted_models = sorted(self.results.items(), key=lambda x: x[1]['Test_RMSE'])\n",
    "        top_models = [name for name, _ in sorted_models[:top_n]]\n",
    "        \n",
    "        param_grids = self.get_hyperparameter_grids()\n",
    "        tuned_results = {}\n",
    "        \n",
    "        for model_name in top_models:\n",
    "            if model_name in param_grids:\n",
    "                print(f\"Tuning {model_name}...\")\n",
    "                \n",
    "                base_model = self.models[model_name]\n",
    "                param_grid = param_grids[model_name]\n",
    "                \n",
    "                # Use RandomizedSearchCV for faster tuning\n",
    "                grid_search = RandomizedSearchCV(\n",
    "                    base_model, param_grid, n_iter=20, cv=5,\n",
    "                    scoring='neg_mean_squared_error', n_jobs=-1,\n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                # Evaluate best model\n",
    "                best_model = grid_search.best_estimator_\n",
    "                \n",
    "                # Get predictions on transformed scale\n",
    "                y_pred_train_transformed = best_model.predict(self.X_train)\n",
    "                y_pred_test_transformed = best_model.predict(self.X_test)\n",
    "                \n",
    "                # Transform predictions back to original scale\n",
    "                y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "                \n",
    "                tuned_results[f'{model_name}_Tuned'] = {\n",
    "                    'Best_Params': grid_search.best_params_,\n",
    "                    'CV_RMSE_transformed': np.sqrt(-grid_search.best_score_),\n",
    "                    'Train_RMSE': np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original)),\n",
    "                    'Test_RMSE': np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original)),\n",
    "                    'Train_R2': r2_score(self.y_train_original, y_pred_train_original),\n",
    "                    'Test_R2': r2_score(self.y_test_original, y_pred_test_original),\n",
    "                    'Train_Custom_Score': self.custom_score_function(self.y_train_original, y_pred_train_original),\n",
    "                    'Test_Custom_Score': self.custom_score_function(self.y_test_original, y_pred_test_original),\n",
    "                    'Model': best_model\n",
    "                }\n",
    "        \n",
    "        self.tuned_results = tuned_results\n",
    "        return tuned_results\n",
    "    \n",
    "    def select_best_model(self):\n",
    "        \"\"\"\n",
    "        Select the best model based on test performance\n",
    "        \"\"\"\n",
    "        print(\"Selecting best model...\")\n",
    "        \n",
    "        # Combine base and tuned results\n",
    "        all_results = {**self.results}\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            all_results.update(self.tuned_results)\n",
    "        \n",
    "        # Find best model based on test custom score\n",
    "        best_model_name = max(all_results.keys(), \n",
    "                            key=lambda x: all_results[x]['Test_Custom_Score'])\n",
    "        \n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_model = all_results[best_model_name]['Model']\n",
    "        self.best_score = all_results[best_model_name]['Test_Custom_Score']\n",
    "        \n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        print(f\"Best Test Custom Score: {self.best_score:.4f}\")\n",
    "        \n",
    "        return self.best_model_name, self.best_model\n",
    "    \n",
    "    def predict(self, X_raw):\n",
    "        \"\"\"\n",
    "        Make predictions on raw data using the complete pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        X_raw: Raw input data (DataFrame) - will be processed through the entire pipeline\n",
    "        \n",
    "        Returns:\n",
    "        y_pred_original: Predictions on original scale\n",
    "        \"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"No model has been trained yet. Run the pipeline first.\")\n",
    "        \n",
    "        if self.imputer is None:\n",
    "            raise ValueError(\"Imputation pipeline not fitted. Run training first.\")\n",
    "        \n",
    "        # Step 1: Fix data types (same as training)\n",
    "        X_fixed = self.fix_data_types(X_raw, \"PREDICTION DATA\")\n",
    "        \n",
    "        # Step 2: Apply imputation\n",
    "        X_imputed = self.imputer.transform(X_fixed)\n",
    "        \n",
    "        # Step 3: Select features\n",
    "        X_features = X_imputed[self.feature_cols].copy()\n",
    "        \n",
    "        # Step 4: Apply categorical encoding\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.label_encoders:\n",
    "                X_features[col] = self.label_encoders[col].transform(X_features[col].astype(str))\n",
    "        \n",
    "        # Step 5: Apply numerical preprocessing\n",
    "        X_processed = self.preprocessor.transform(X_features)\n",
    "        \n",
    "        # Convert back to DataFrame for consistency\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_processed = pd.DataFrame(X_processed, columns=feature_names)\n",
    "        \n",
    "        # Step 6: Get predictions on transformed scale\n",
    "        y_pred_transformed = self.best_model.predict(X_processed)\n",
    "        \n",
    "        # Step 7: Transform back to original scale\n",
    "        y_pred_original = self.inverse_transform_predictions(y_pred_transformed)\n",
    "        \n",
    "        return y_pred_original\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive results summary\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL SELECTION RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Base models results\n",
    "        print(\"\\nBASE MODELS PERFORMANCE (on original scale):\")\n",
    "        print(\"-\" * 50)\n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('Test_Custom_Score', ascending=False)\n",
    "        \n",
    "        display_cols = ['Test_Custom_Score', 'Test_RMSE', 'Test_R2', 'CV_RMSE_transformed']\n",
    "        print(results_df[display_cols].round(4).to_string())\n",
    "        \n",
    "        # Tuned models results\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            print(\"\\nTUNED MODELS PERFORMANCE (on original scale):\")\n",
    "            print(\"-\" * 50)\n",
    "            tuned_df = pd.DataFrame(self.tuned_results).T\n",
    "            tuned_df = tuned_df.sort_values('Test_Custom_Score', ascending=False)\n",
    "            print(tuned_df[display_cols].round(4).to_string())\n",
    "        \n",
    "        print(f\"\\nBEST MODEL: {self.best_model_name}\")\n",
    "        print(f\"BEST SCORE: {self.best_score:.4f}\")\n",
    "        print(\"\\nNote: All metrics except CV_RMSE_transformed are calculated on original scale\")\n",
    "        \n",
    "    def save_best_model(self, filepath='best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the best model and all preprocessing components\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.best_model,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'target_transformer': self.target_transformer,\n",
    "            'imputer': self.imputer,  # Include the fitted imputation pipeline\n",
    "            'feature_names': self.feature_cols,\n",
    "            'categorical_cols': self.categorical_cols,\n",
    "            'numerical_cols': self.numerical_cols,\n",
    "            'best_model_name': self.best_model_name,\n",
    "            'best_score': self.best_score\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"Best model with complete pipeline saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath='model/best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Load a saved model with complete pipeline\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_package = pickle.load(f)\n",
    "        \n",
    "        self.best_model = model_package['model']\n",
    "        self.preprocessor = model_package['preprocessor']\n",
    "        self.label_encoders = model_package['label_encoders']\n",
    "        self.target_transformer = model_package['target_transformer']\n",
    "        self.imputer = model_package['imputer']  # Load the imputation pipeline\n",
    "        self.feature_cols = model_package['feature_names']\n",
    "        self.categorical_cols = model_package['categorical_cols']\n",
    "        self.numerical_cols = model_package['numerical_cols']\n",
    "        self.best_model_name = model_package.get('best_model_name', 'Unknown')\n",
    "        self.best_score = model_package.get('best_score', 0)\n",
    "        \n",
    "        print(f\"Model with complete pipeline loaded successfully: {self.best_model_name}\")\n",
    "        \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete model selection pipeline including data type fixing and imputation\n",
    "        \"\"\"\n",
    "        print(\"Starting Solar Panel Performance Model Selection Pipeline...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load raw data, fix data types, and apply imputation\n",
    "        self.load_and_prepare_data()\n",
    "        \n",
    "        # Step 2: Create preprocessing pipeline\n",
    "        self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Step 3: Prepare train-test split\n",
    "        self.prepare_train_test_split()\n",
    "        \n",
    "        # Step 4: Define models\n",
    "        self.define_models()\n",
    "        \n",
    "        # Step 5: Evaluate base models\n",
    "        self.evaluate_base_models()\n",
    "        \n",
    "        # Step 6: Hyperparameter tuning\n",
    "        self.hyperparameter_tuning()\n",
    "        \n",
    "        # Step 7: Select best model\n",
    "        self.select_best_model()\n",
    "        \n",
    "        # Step 8: Print results\n",
    "        self.print_results_summary()\n",
    "        \n",
    "        # Step 9: Save best model with complete pipeline\n",
    "        self.save_best_model()\n",
    "        \n",
    "        return self.best_model, self.best_model_name, self.best_score\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model selector\n",
    "    # Note: Use raw data path here, not pre-engineered data\n",
    "    selector = SolarPanelModelSelector(data_path='dataset/train.csv')\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    best_model, best_model_name, best_score = selector.run_complete_pipeline()\n",
    "    \n",
    "    print(f\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Best model: {best_model_name} with score: {best_score:.4f}\")\n",
    "    \n",
    "    # Example of making predictions on new raw data\n",
    "    # new_raw_data = pd.read_csv('new_raw_data.csv')\n",
    "    # predictions = selector.predict(new_raw_data)  # This will handle the complete pipeline\n",
    "    \n",
    "    # Example of loading and using a saved model\n",
    "    # new_selector = SolarPanelModelSelector()\n",
    "    # new_selector.load_model('best_solar_model.pkl')\n",
    "    # predictions = new_selector.predict(new_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7f432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09292886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c355dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Solar Panel Performance Model Selection Pipeline...\n",
      "============================================================\n",
      "Loading raw data...\n",
      "Raw dataset shape: (20000, 17)\n",
      "Missing values in raw data:\n",
      "temperature           1001\n",
      "irradiance             987\n",
      "panel_age             1011\n",
      "maintenance_count     1027\n",
      "soiling_ratio         1010\n",
      "voltage                993\n",
      "current                977\n",
      "module_temperature     978\n",
      "cloud_coverage        1010\n",
      "error_code            5912\n",
      "installation_type     5028\n",
      "dtype: int64\n",
      "\n",
      "Step 1: Fixing data types...\n",
      "\n",
      "=== FIXING DATA TYPES FOR TRAINING DATA ===\n",
      "\n",
      "Processing humidity:\n",
      "Original dtype: object\n",
      "Non-numeric values found in humidity:\n",
      "humidity\n",
      "unknown    50\n",
      "error      40\n",
      "badval     37\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 127\n",
      "Valid numeric values: 19873\n",
      "Min: 0.011\n",
      "Max: 99.995\n",
      "Mean: 50.066\n",
      "\n",
      "Processing wind_speed:\n",
      "Original dtype: object\n",
      "Non-numeric values found in wind_speed:\n",
      "wind_speed\n",
      "badval     42\n",
      "error      41\n",
      "unknown    36\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 119\n",
      "Valid numeric values: 19881\n",
      "Min: 0.001\n",
      "Max: 14.999\n",
      "Mean: 7.413\n",
      "\n",
      "Processing pressure:\n",
      "Original dtype: object\n",
      "Non-numeric values found in pressure:\n",
      "pressure\n",
      "unknown    46\n",
      "error      45\n",
      "badval     44\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 135\n",
      "Valid numeric values: 19865\n",
      "Min: 970.087\n",
      "Max: 1052.866\n",
      "Mean: 1012.981\n",
      "\n",
      "=== DATA TYPE VERIFICATION ===\n",
      "Data types after fixing:\n",
      "humidity: float64\n",
      "wind_speed: float64\n",
      "pressure: float64\n",
      "\n",
      "Step 2: Applying imputation pipeline...\n",
      "Dataset shape after imputation: (20000, 17)\n",
      "Remaining missing values after imputation: 0\n",
      "\n",
      "Step 3: Creating engineered features...\n",
      "Dataset shape after feature engineering: (20000, 31)\n",
      "\n",
      "Step 4: Dropping features: ['soiling_loss', 'temp_difference', 'installation_type_tracking', 'pressure', 'wind_cooling_effect', 'id', 'voltage', 'current', 'temperature', 'module_temperature', 'irradiance', 'wind_speed', 'panel_age', 'cloud_coverage', 'soiling_ratio', 'maintenance_count', 'humidity']\n",
      "Dropped features: ['soiling_loss', 'temp_difference', 'installation_type_tracking', 'pressure', 'wind_cooling_effect', 'id', 'voltage', 'current', 'temperature', 'module_temperature', 'irradiance', 'wind_speed', 'panel_age', 'cloud_coverage', 'soiling_ratio', 'maintenance_count', 'humidity']\n",
      "Dataset shape after dropping features: (20000, 14)\n",
      "Categorical columns: ['string_id', 'error_code', 'installation_type']\n",
      "Numerical columns: ['power_output', 'irradiance_normalized', 'temp_coefficient_effect', 'expected_irradiance_clean', 'irradiance_cloud_ratio', 'age_degradation_factor', 'maintenance_frequency', 'environmental_stress', 'effective_module_temp']\n",
      "Total features: 13\n",
      "Creating preprocessing pipeline...\n",
      "Preparing train-test split...\n",
      "Training set shape: (16000, 12)\n",
      "Test set shape: (4000, 12)\n",
      "Defining models...\n",
      "Evaluating base models...\n",
      "Training Linear Regression...\n",
      "  ✓ Linear Regression completed - Test Custom Score: 89.2771\n",
      "Training Ridge...\n",
      "  ✓ Ridge completed - Test Custom Score: 89.2772\n",
      "Training Lasso...\n",
      "  ✓ Lasso completed - Test Custom Score: 85.7084\n",
      "Training ElasticNet...\n",
      "  ✓ ElasticNet completed - Test Custom Score: 86.3594\n",
      "Training Decision Tree...\n",
      "  ✓ Decision Tree completed - Test Custom Score: 84.7013\n",
      "Training Random Forest...\n",
      "  ✓ Random Forest completed - Test Custom Score: 89.2470\n",
      "Training Extra Trees...\n",
      "  ✓ Extra Trees completed - Test Custom Score: 89.2365\n",
      "Training Gradient Boosting...\n",
      "  ✓ Gradient Boosting completed - Test Custom Score: 89.3570\n",
      "Training XGBoost...\n",
      "  ✓ XGBoost completed - Test Custom Score: 89.0774\n",
      "Training LightGBM...\n",
      "  ✓ LightGBM completed - Test Custom Score: 89.3617\n",
      "Training CatBoost...\n",
      "  ✓ CatBoost completed - Test Custom Score: 89.3584\n",
      "Training KNN...\n",
      "  ✓ KNN completed - Test Custom Score: 88.0437\n",
      "Training SVR...\n",
      "  ✓ SVR completed - Test Custom Score: 89.2108\n",
      "Training ANN...\n",
      "  ✓ ANN completed - Test Custom Score: 89.4433\n",
      "Performing hyperparameter tuning for top 5 models...\n",
      "Tuning ANN...\n",
      "  ✓ ANN tuning completed\n",
      "Tuning LightGBM...\n",
      "  ✓ LightGBM tuning completed\n",
      "Tuning Gradient Boosting...\n",
      "  ✓ Gradient Boosting tuning completed\n",
      "Tuning Ridge...\n",
      "  ✓ Ridge tuning completed\n",
      "Selecting best model...\n",
      "Best Model: ANN\n",
      "Best Test Custom Score: 89.4433\n",
      "\n",
      "================================================================================\n",
      "MODEL SELECTION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "BASE MODELS PERFORMANCE (on original scale):\n",
      "--------------------------------------------------\n",
      "                  Test_Custom_Score Test_RMSE   Test_R2 CV_RMSE_transformed\n",
      "ANN                       89.443268  0.105567  0.445653            0.616186\n",
      "LightGBM                  89.361724  0.106383  0.437056               0.624\n",
      "CatBoost                  89.358395  0.106416  0.436704            0.621691\n",
      "Gradient Boosting         89.356999   0.10643  0.436556            0.621646\n",
      "Ridge                     89.277198  0.107228  0.428075             0.62569\n",
      "Linear Regression         89.277066  0.107229  0.428061             0.62569\n",
      "Random Forest             89.246992   0.10753  0.424848            0.632914\n",
      "Extra Trees                89.23653  0.107635  0.423729            0.634087\n",
      "SVR                       89.210751  0.107892  0.420965            0.623475\n",
      "XGBoost                    89.07745  0.109226  0.406568            0.653564\n",
      "KNN                       88.043736  0.119563  0.288928            0.772467\n",
      "ElasticNet                86.359375  0.136406  0.074469            0.939797\n",
      "Lasso                     85.708352  0.142916 -0.015985            1.000366\n",
      "Decision Tree              84.70134  0.152987 -0.164205            0.923286\n",
      "\n",
      "TUNED MODELS PERFORMANCE (on original scale):\n",
      "--------------------------------------------------\n",
      "                        Test_Custom_Score Test_RMSE   Test_R2 CV_RMSE_transformed\n",
      "ANN_Tuned                       89.440461  0.105595  0.445358             0.61235\n",
      "LightGBM_Tuned                  89.363966   0.10636  0.437293            0.619537\n",
      "Gradient Boosting_Tuned         89.347362  0.106526  0.435535            0.620737\n",
      "Ridge_Tuned                     89.277079  0.107229  0.428062             0.62569\n",
      "\n",
      "BEST MODEL: ANN\n",
      "BEST SCORE: 89.4433\n",
      "\n",
      "Note: All metrics except CV_RMSE_transformed are calculated on original scale\n",
      "Best model with complete pipeline saved to best_solar_model.pkl\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Best model: ANN with score: 89.4433\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Neural Network imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from utils.imputation import ImputationPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class ANNRegressor:\n",
    "    \"\"\"\n",
    "    Custom ANN Regressor wrapper that mimics scikit-learn interface\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons=128, layers=3, dropout_rate=0.3, \n",
    "                 learning_rate=0.001, l1_reg=0.0, l2_reg=0.01,\n",
    "                 epochs=200, batch_size=32, validation_split=0.2,\n",
    "                 patience=20, verbose=0):\n",
    "        self.neurons = neurons\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "        self.history_ = None\n",
    "        \n",
    "    def _build_model(self, input_dim):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(self.neurons, \n",
    "                       input_dim=input_dim,\n",
    "                       activation='relu',\n",
    "                       kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.layers - 1):\n",
    "            # Gradually decrease neurons in deeper layers\n",
    "            layer_neurons = max(self.neurons // (2 ** i), 32)\n",
    "            model.add(Dense(layer_neurons,\n",
    "                           activation='relu',\n",
    "                           kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the neural network\"\"\"\n",
    "        # Convert to numpy arrays if needed\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Ensure y is 1D\n",
    "        if len(y.shape) > 1:\n",
    "            y = y.flatten()\n",
    "        \n",
    "        # Build model\n",
    "        self.model_ = self._build_model(X.shape[1])\n",
    "        \n",
    "        # Set up callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=self.patience, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(patience=self.patience//2, factor=0.5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history_ = self.model_.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        # Convert to numpy array if needed\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        predictions = self.model_.predict(X, verbose=0)\n",
    "        return predictions.flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'neurons': self.neurons,\n",
    "            'layers': self.layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'l1_reg': self.l1_reg,\n",
    "            'l2_reg': self.l2_reg,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'validation_split': self.validation_split,\n",
    "            'patience': self.patience,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "class SolarPanelModelSelector:\n",
    "    def __init__(self, data_path='dataset/train.csv', test_size=0.2, random_state=42, features_to_drop=None):\n",
    "        \"\"\"\n",
    "        Initialize the model selector with data loading and basic setup\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.preprocessor = None\n",
    "        self.imputer = None\n",
    "        self.features_to_drop = features_to_drop or []\n",
    "        \n",
    "        \n",
    "    def fix_data_types(self, df, dataset_name):\n",
    "        \"\"\"Fix data type inconsistencies for specific columns\"\"\"\n",
    "        df_fixed = df.copy()\n",
    "        \n",
    "        # Define columns that should be numeric\n",
    "        numeric_columns_to_fix = ['humidity', 'wind_speed', 'pressure']\n",
    "        \n",
    "        print(f\"\\n=== FIXING DATA TYPES FOR {dataset_name} ===\")\n",
    "        \n",
    "        for col in numeric_columns_to_fix:\n",
    "            if col in df_fixed.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                print(f\"Original dtype: {df_fixed[col].dtype}\")\n",
    "                \n",
    "                # Check for non-numeric values before conversion\n",
    "                if df_fixed[col].dtype == 'object':\n",
    "                    # Display unique non-numeric values\n",
    "                    try:\n",
    "                        # Try to convert to numeric and see what fails\n",
    "                        numeric_conversion = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        non_numeric_mask = pd.isna(numeric_conversion) & df_fixed[col].notna()\n",
    "                        \n",
    "                        if non_numeric_mask.any():\n",
    "                            print(f\"Non-numeric values found in {col}:\")\n",
    "                            non_numeric_values = df_fixed.loc[non_numeric_mask, col].value_counts()\n",
    "                            print(non_numeric_values.head(10))\n",
    "                            \n",
    "                            # Handle common non-numeric patterns\n",
    "                            df_fixed[col] = df_fixed[col].astype(str)\n",
    "                            \n",
    "                            # Remove common problematic characters\n",
    "                            df_fixed[col] = df_fixed[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "                            df_fixed[col] = df_fixed[col].str.strip()\n",
    "                            \n",
    "                            # Handle empty strings\n",
    "                            df_fixed[col] = df_fixed[col].replace('', np.nan)\n",
    "                            df_fixed[col] = df_fixed[col].replace('nan', np.nan)\n",
    "                            \n",
    "                        # Convert to numeric\n",
    "                        df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        \n",
    "                        print(f\"Converted dtype: {df_fixed[col].dtype}\")\n",
    "                        print(f\"Missing values after conversion: {df_fixed[col].isnull().sum()}\")\n",
    "                        print(f\"Valid numeric values: {df_fixed[col].notna().sum()}\")\n",
    "                        \n",
    "                        # Basic statistics for converted column\n",
    "                        if df_fixed[col].notna().any():\n",
    "                            print(f\"Min: {df_fixed[col].min():.3f}\")\n",
    "                            print(f\"Max: {df_fixed[col].max():.3f}\")\n",
    "                            print(f\"Mean: {df_fixed[col].mean():.3f}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting {col}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"{col} is already numeric type: {df_fixed[col].dtype}\")\n",
    "        \n",
    "        return df_fixed\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data, fix data types, and apply imputation pipeline\n",
    "        \"\"\"\n",
    "        print(\"Loading raw data...\")\n",
    "        self.df_raw = pd.read_csv(self.data_path)\n",
    "        print(f\"Raw dataset shape: {self.df_raw.shape}\")\n",
    "        print(f\"Missing values in raw data:\\n{self.df_raw.isnull().sum()[self.df_raw.isnull().sum() > 0]}\")\n",
    "        \n",
    "        # Step 1: Fix data types BEFORE imputation\n",
    "        print(\"\\nStep 1: Fixing data types...\")\n",
    "        self.df_fixed = self.fix_data_types(self.df_raw, \"TRAINING DATA\")\n",
    "        \n",
    "        # Verify the fixes\n",
    "        print(\"\\n=== DATA TYPE VERIFICATION ===\")\n",
    "        print(\"Data types after fixing:\")\n",
    "        for col in ['humidity', 'wind_speed', 'pressure']:\n",
    "            if col in self.df_fixed.columns:\n",
    "                print(f\"{col}: {self.df_fixed[col].dtype}\")\n",
    "        \n",
    "        # Step 2: Initialize and apply imputation pipeline (WITHOUT feature creation)\n",
    "        print(\"\\nStep 2: Applying imputation pipeline...\")\n",
    "        self.imputer = ImputationPipeline()\n",
    "        self.df_imputed = self.imputer.fit_transform(self.df_fixed)\n",
    "        \n",
    "        print(f\"Dataset shape after imputation: {self.df_imputed.shape}\")\n",
    "        remaining_missing = self.df_imputed.isnull().sum().sum()\n",
    "        print(f\"Remaining missing values after imputation: {remaining_missing}\")\n",
    "        \n",
    "        # Step 3: NOW create features AFTER imputation is complete\n",
    "        print(\"\\nStep 3: Creating engineered features...\")\n",
    "        from utils.feature_engineering import SolarFeatureEngineering\n",
    "        feature_engineer = SolarFeatureEngineering()\n",
    "        self.df = feature_engineer.create_solar_features(self.df_imputed)\n",
    "        print(f\"Dataset shape after feature engineering: {self.df.shape}\")\n",
    "        \n",
    "        # Verify no missing values in new features\n",
    "        new_missing = self.df.isnull().sum().sum()\n",
    "        if new_missing > 0:\n",
    "            print(f\"Warning: {new_missing} missing values found after feature engineering\")\n",
    "            print(\"Missing values by column:\")\n",
    "            print(self.df.isnull().sum()[self.df.isnull().sum() > 0])\n",
    "\n",
    "        # Step 4: Drop selected features if specified\n",
    "        if self.features_to_drop:\n",
    "            print(f\"\\nStep 4: Dropping features: {self.features_to_drop}\")\n",
    "            available_features = [col for col in self.features_to_drop if col in self.df.columns]\n",
    "            unavailable_features = [col for col in self.features_to_drop if col not in self.df.columns]\n",
    "            \n",
    "            if available_features:\n",
    "                self.df = self.df.drop(columns=available_features)\n",
    "                print(f\"Dropped features: {available_features}\")\n",
    "            \n",
    "            if unavailable_features:\n",
    "                print(f\"Warning: Features not found in dataset: {unavailable_features}\")\n",
    "            \n",
    "            print(f\"Dataset shape after dropping features: {self.df.shape}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        self.target_col = 'efficiency'\n",
    "        self.feature_cols = [col for col in self.df.columns if col != self.target_col]\n",
    "        \n",
    "        # Identify categorical and numerical columns\n",
    "        self.categorical_cols = self.df[self.feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "        self.numerical_cols = self.df[self.feature_cols].select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        \n",
    "        print(f\"Categorical columns: {self.categorical_cols}\")\n",
    "        print(f\"Numerical columns: {self.numerical_cols}\")\n",
    "        print(f\"Total features: {len(self.feature_cols)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for numerical and categorical features\n",
    "        \"\"\"\n",
    "        print(\"Creating preprocessing pipeline...\")\n",
    "        \n",
    "        # For neural networks, we need StandardScaler instead of RobustScaler\n",
    "        # as neural networks work better with standardized inputs\n",
    "        numerical_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler())  # Neural networks prefer StandardScaler\n",
    "        ])\n",
    "        \n",
    "        # Categorical preprocessing pipeline\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('encoder', 'passthrough')  # Will be handled separately\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('num', numerical_pipeline, self.numerical_cols),\n",
    "            ('cat', categorical_pipeline, self.categorical_cols)\n",
    "        ])\n",
    "        \n",
    "        return self.preprocessor\n",
    "    \n",
    "    def prepare_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Prepare train-test split with proper preprocessing\n",
    "        \"\"\"\n",
    "        print(\"Preparing train-test split...\")\n",
    "        \n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        y = self.df[self.target_col].copy()\n",
    "        \n",
    "        # Store original target values for later use\n",
    "        self.y_original = y.copy()\n",
    "        \n",
    "        # Apply power transformation to target if it's skewed\n",
    "        self.target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "        y_transformed = self.target_transformer.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_transformed, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Also split original target for evaluation\n",
    "        _, _, self.y_train_original, self.y_test_original = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Handle categorical encoding\n",
    "        self.label_encoders = {}\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = le.transform(X_test[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        \n",
    "        # Apply numerical preprocessing\n",
    "        X_train_scaled = self.preprocessor.fit_transform(X_train)\n",
    "        X_test_scaled = self.preprocessor.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train_scaled, X_test_scaled\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        \n",
    "        print(f\"Training set shape: {self.X_train.shape}\")\n",
    "        print(f\"Test set shape: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def define_models(self):\n",
    "        \"\"\"\n",
    "        Define all models to be tested including ANN\n",
    "        \"\"\"\n",
    "        print(\"Defining models...\")\n",
    "        \n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(random_state=self.random_state),\n",
    "            'Lasso': Lasso(random_state=self.random_state),\n",
    "            'ElasticNet': ElasticNet(random_state=self.random_state),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=self.random_state),\n",
    "            'Random Forest': RandomForestRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Extra Trees': ExtraTreesRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=self.random_state),\n",
    "            'XGBoost': XGBRegressor(random_state=self.random_state, eval_metric='rmse'),\n",
    "            'LightGBM': LGBMRegressor(random_state=self.random_state, verbose=-1),\n",
    "            'CatBoost': CatBoostRegressor(random_state=self.random_state, verbose=False),\n",
    "            'KNN': KNeighborsRegressor(),\n",
    "            'SVR': SVR(),\n",
    "            'ANN': ANNRegressor(verbose=0)  # Use custom ANN wrapper\n",
    "        }\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_pred):\n",
    "        \"\"\"\n",
    "        Apply inverse transformation to predictions to get them back to original scale\n",
    "        \"\"\"\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_pred_original = self.target_transformer.inverse_transform(y_pred_reshaped).flatten()\n",
    "        return y_pred_original\n",
    "    \n",
    "    def custom_score_function(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Custom scoring function as per problem statement\n",
    "        Score = 100*(1-sqrt(MSE))\n",
    "        Note: This should be calculated on original scale, not transformed scale\n",
    "        \"\"\"\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        score = 100 * (1 - np.sqrt(mse))\n",
    "        return score\n",
    "    \n",
    "    def evaluate_base_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all base models using cross-validation\n",
    "        \"\"\"\n",
    "        print(\"Evaluating base models...\")\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Cross-validation scores (on transformed target)\n",
    "                if name == 'ANN':\n",
    "                    # For ANN, use fewer CV folds due to computational cost\n",
    "                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3, \n",
    "                                              scoring='neg_mean_squared_error', n_jobs=1)\n",
    "                else:\n",
    "                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, \n",
    "                                              scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "                \n",
    "                # Fit model for additional metrics\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                # Get predictions on transformed scale\n",
    "                y_pred_train_transformed = model.predict(self.X_train)\n",
    "                y_pred_test_transformed = model.predict(self.X_test)\n",
    "                \n",
    "                # Transform predictions back to original scale\n",
    "                y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "                \n",
    "                # Calculate metrics on ORIGINAL scale\n",
    "                train_rmse = np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original))\n",
    "                test_rmse = np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original))\n",
    "                train_r2 = r2_score(self.y_train_original, y_pred_train_original)\n",
    "                test_r2 = r2_score(self.y_test_original, y_pred_test_original)\n",
    "                \n",
    "                # Custom score on original scale\n",
    "                train_custom_score = self.custom_score_function(self.y_train_original, y_pred_train_original)\n",
    "                test_custom_score = self.custom_score_function(self.y_test_original, y_pred_test_original)\n",
    "                \n",
    "                # CV RMSE on transformed scale (for comparison)\n",
    "                cv_rmse_transformed = np.sqrt(-cv_scores.mean())\n",
    "                \n",
    "                self.results[name] = {\n",
    "                    'CV_RMSE_transformed': cv_rmse_transformed,\n",
    "                    'CV_RMSE_std': np.sqrt(cv_scores.std()),\n",
    "                    'Train_RMSE': train_rmse,\n",
    "                    'Test_RMSE': test_rmse,\n",
    "                    'Train_R2': train_r2,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Train_Custom_Score': train_custom_score,\n",
    "                    'Test_Custom_Score': test_custom_score,\n",
    "                    'Model': model\n",
    "                }\n",
    "                \n",
    "                print(f\"  ✓ {name} completed - Test Custom Score: {test_custom_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error training {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_hyperparameter_grids(self):\n",
    "        \"\"\"\n",
    "        Define hyperparameter grids for top performing models including ANN\n",
    "        \"\"\"\n",
    "        param_grids = {\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "            },\n",
    "            'ANN': {\n",
    "                'neurons': [64, 128, 256],\n",
    "                'layers': [2, 3, 4],\n",
    "                'dropout_rate': [0.2, 0.3, 0.4],\n",
    "                'learning_rate': [0.001, 0.01],\n",
    "                'l2_reg': [0.001, 0.01, 0.1],\n",
    "                'epochs': [150, 250],\n",
    "                'batch_size': [16, 32, 64]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return param_grids\n",
    "    \n",
    "    def hyperparameter_tuning(self, top_n=5):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning for top N models including ANN\n",
    "        \"\"\"\n",
    "        print(f\"Performing hyperparameter tuning for top {top_n} models...\")\n",
    "        \n",
    "        # Sort models by test RMSE\n",
    "        sorted_models = sorted(self.results.items(), key=lambda x: x[1]['Test_RMSE'])\n",
    "        top_models = [name for name, _ in sorted_models[:top_n]]\n",
    "        \n",
    "        param_grids = self.get_hyperparameter_grids()\n",
    "        tuned_results = {}\n",
    "        \n",
    "        for model_name in top_models:\n",
    "            if model_name in param_grids:\n",
    "                print(f\"Tuning {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    base_model = self.models[model_name]\n",
    "                    param_grid = param_grids[model_name]\n",
    "                    \n",
    "                    # Use different search strategies for different models\n",
    "                    if model_name == 'ANN':\n",
    "                        # Use fewer iterations for ANN due to computational cost\n",
    "                        grid_search = RandomizedSearchCV(\n",
    "                            base_model, param_grid, n_iter=10, cv=3,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=1,\n",
    "                            random_state=self.random_state\n",
    "                        )\n",
    "                    else:\n",
    "                        # Use RandomizedSearchCV for faster tuning\n",
    "                        grid_search = RandomizedSearchCV(\n",
    "                            base_model, param_grid, n_iter=20, cv=5,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=-1,\n",
    "                            random_state=self.random_state\n",
    "                        )\n",
    "                    \n",
    "                    grid_search.fit(self.X_train, self.y_train)\n",
    "                    \n",
    "                    # Evaluate best model\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    \n",
    "                    # Get predictions on transformed scale\n",
    "                    y_pred_train_transformed = best_model.predict(self.X_train)\n",
    "                    y_pred_test_transformed = best_model.predict(self.X_test)\n",
    "                    \n",
    "                    # Transform predictions back to original scale\n",
    "                    y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                    y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "                    \n",
    "                    tuned_results[f'{model_name}_Tuned'] = {\n",
    "                        'Best_Params': grid_search.best_params_,\n",
    "                        'CV_RMSE_transformed': np.sqrt(-grid_search.best_score_),\n",
    "                        'Train_RMSE': np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original)),\n",
    "                        'Test_RMSE': np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original)),\n",
    "                        'Train_R2': r2_score(self.y_train_original, y_pred_train_original),\n",
    "                        'Test_R2': r2_score(self.y_test_original, y_pred_test_original),\n",
    "                        'Train_Custom_Score': self.custom_score_function(self.y_train_original, y_pred_train_original),\n",
    "                        'Test_Custom_Score': self.custom_score_function(self.y_test_original, y_pred_test_original),\n",
    "                        'Model': best_model\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  ✓ {model_name} tuning completed\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error tuning {model_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        self.tuned_results = tuned_results\n",
    "        return tuned_results\n",
    "    \n",
    "    def select_best_model(self):\n",
    "        \"\"\"\n",
    "        Select the best model based on test performance\n",
    "        \"\"\"\n",
    "        print(\"Selecting best model...\")\n",
    "        \n",
    "        # Combine base and tuned results\n",
    "        all_results = {**self.results}\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            all_results.update(self.tuned_results)\n",
    "        \n",
    "        # Find best model based on test custom score\n",
    "        best_model_name = max(all_results.keys(), \n",
    "                            key=lambda x: all_results[x]['Test_Custom_Score'])\n",
    "        \n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_model = all_results[best_model_name]['Model']\n",
    "        self.best_score = all_results[best_model_name]['Test_Custom_Score']\n",
    "        \n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        print(f\"Best Test Custom Score: {self.best_score:.4f}\")\n",
    "        \n",
    "        return self.best_model_name, self.best_model\n",
    "    \n",
    "    def predict(self, X_raw):\n",
    "        \"\"\"\n",
    "        Make predictions on raw data using the complete pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        X_raw: Raw input data (DataFrame) - will be processed through the entire pipeline\n",
    "        \n",
    "        Returns:\n",
    "        y_pred_original: Predictions on original scale\n",
    "        \"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"No model has been trained yet. Run the pipeline first.\")\n",
    "        \n",
    "        if self.imputer is None:\n",
    "            raise ValueError(\"Imputation pipeline not fitted. Run training first.\")\n",
    "        \n",
    "        # Step 1: Fix data types (same as training)\n",
    "        X_fixed = self.fix_data_types(X_raw, \"PREDICTION DATA\")\n",
    "        \n",
    "        # Step 2: Apply imputation\n",
    "        X_imputed = self.imputer.transform(X_fixed)\n",
    "\n",
    "        # Step 2.5: Drop the same features as during training\n",
    "        if self.features_to_drop:\n",
    "            available_features = [col for col in self.features_to_drop if col in X_imputed.columns]\n",
    "            if available_features:\n",
    "                X_imputed = X_imputed.drop(columns=available_features)\n",
    "        \n",
    "        # Step 3: Select features\n",
    "        X_features = X_imputed[self.feature_cols].copy()\n",
    "        \n",
    "        # Step 4: Apply categorical encoding\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.label_encoders:\n",
    "                X_features[col] = self.label_encoders[col].transform(X_features[col].astype(str))\n",
    "        \n",
    "        # Step 5: Apply numerical preprocessing\n",
    "        X_processed = self.preprocessor.transform(X_features)\n",
    "        \n",
    "        # Convert back to DataFrame for consistency\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_processed = pd.DataFrame(X_processed, columns=feature_names)\n",
    "        \n",
    "        # Step 6: Get predictions on transformed scale\n",
    "        y_pred_transformed = self.best_model.predict(X_processed)\n",
    "        \n",
    "        # Step 7: Transform back to original scale\n",
    "        y_pred_original = self.inverse_transform_predictions(y_pred_transformed)\n",
    "        \n",
    "        return y_pred_original\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive results summary\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL SELECTION RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Base models results\n",
    "        print(\"\\nBASE MODELS PERFORMANCE (on original scale):\")\n",
    "        print(\"-\" * 50)\n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('Test_Custom_Score', ascending=False)\n",
    "        \n",
    "        display_cols = ['Test_Custom_Score', 'Test_RMSE', 'Test_R2', 'CV_RMSE_transformed']\n",
    "        print(results_df[display_cols].round(4).to_string())\n",
    "        \n",
    "        # Tuned models results\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            print(\"\\nTUNED MODELS PERFORMANCE (on original scale):\")\n",
    "            print(\"-\" * 50)\n",
    "            tuned_df = pd.DataFrame(self.tuned_results).T\n",
    "            tuned_df = tuned_df.sort_values('Test_Custom_Score', ascending=False)\n",
    "            print(tuned_df[display_cols].round(4).to_string())\n",
    "        \n",
    "        print(f\"\\nBEST MODEL: {self.best_model_name}\")\n",
    "        print(f\"BEST SCORE: {self.best_score:.4f}\")\n",
    "        print(\"\\nNote: All metrics except CV_RMSE_transformed are calculated on original scale\")\n",
    "        \n",
    "    def save_best_model(self, filepath='model/best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the best model and all preprocessing components\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.best_model,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'target_transformer': self.target_transformer,\n",
    "            'imputer': self.imputer,  # Include the fitted imputation pipeline\n",
    "            'feature_names': self.feature_cols,\n",
    "            'categorical_cols': self.categorical_cols,\n",
    "            'numerical_cols': self.numerical_cols,\n",
    "            'best_model_name': self.best_model_name,\n",
    "            'best_score': self.best_score,\n",
    "            'features_to_drop': self.features_to_drop\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"Best model with complete pipeline saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath='best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Load a saved model with complete pipeline\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_package = pickle.load(f)\n",
    "        \n",
    "        self.best_model = model_package['model']\n",
    "        self.preprocessor = model_package['preprocessor']\n",
    "        self.label_encoders = model_package['label_encoders']\n",
    "        self.target_transformer = model_package['target_transformer']\n",
    "        self.imputer = model_package['imputer']  # Load the imputation pipeline\n",
    "        self.feature_cols = model_package['feature_names']\n",
    "        self.categorical_cols = model_package['categorical_cols']\n",
    "        self.numerical_cols = model_package['numerical_cols']\n",
    "        self.best_model_name = model_package.get('best_model_name', 'Unknown')\n",
    "        self.best_score = model_package.get('best_score', 0)\n",
    "        self.features_to_drop = model_package.get('features_to_drop', []) \n",
    "        \n",
    "        print(f\"Model with complete pipeline loaded successfully: {self.best_model_name}\")\n",
    "        \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete model selection pipeline including data type fixing and imputation\n",
    "        \"\"\"\n",
    "        print(\"Starting Solar Panel Performance Model Selection Pipeline...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load raw data, fix data types, and apply imputation\n",
    "        self.load_and_prepare_data()\n",
    "        \n",
    "        # Step 2: Create preprocessing pipeline\n",
    "        self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Step 3: Prepare train-test split\n",
    "        self.prepare_train_test_split()\n",
    "        \n",
    "        # Step 4: Define models\n",
    "        self.define_models()\n",
    "        \n",
    "        # Step 5: Evaluate base models\n",
    "        self.evaluate_base_models()\n",
    "        \n",
    "        # Step 6: Hyperparameter tuning\n",
    "        self.hyperparameter_tuning()\n",
    "        \n",
    "        # Step 7: Select best model\n",
    "        self.select_best_model()\n",
    "        \n",
    "        # Step 8: Print results\n",
    "        self.print_results_summary()\n",
    "        \n",
    "        # Step 9: Save best model with complete pipeline\n",
    "        self.save_best_model()\n",
    "        \n",
    "        return self.best_model, self.best_model_name, self.best_score\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define features to drop\n",
    "    features_to_drop = [\n",
    "        'soiling_loss',\n",
    "        'temp_difference', \n",
    "        'installation_type_tracking',\n",
    "        'pressure',\n",
    "        'wind_cooling_effect',\n",
    "        'id', 'voltage','current', 'temperature',\n",
    "        'module_temperature', 'irradiance', 'wind_speed',\n",
    "        'panel_age', 'cloud_coverage', \n",
    "        'soiling_ratio', 'maintenance_count', 'humidity'\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Initialize the model selector\n",
    "    # Note: Use raw data path here, not pre-engineered data\n",
    "    selector = SolarPanelModelSelector(data_path='dataset/train.csv', features_to_drop=features_to_drop)\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    best_model, best_model_name, best_score = selector.run_complete_pipeline()\n",
    "    \n",
    "    print(f\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Best model: {best_model_name} with score: {best_score:.4f}\")\n",
    "    \n",
    "    # Example of making predictions on new raw data\n",
    "    # new_raw_data = pd.read_csv('new_raw_data.csv')\n",
    "    # predictions = selector.predict(new_raw_data)  # This will handle the complete pipeline\n",
    "    \n",
    "    # Example of loading and using a saved model\n",
    "    # new_selector = SolarPanelModelSelector()\n",
    "    # new_selector.load_model('best_solar_model.pkl')\n",
    "    # predictions = new_selector.predict(new_raw_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
