{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b88b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Solar Panel Prediction Pipeline...\n",
      "Model loaded successfully: Stacking_Linear\n",
      "Model score: 89.3705\n",
      "Expected features: 13\n",
      "Features to drop: ['soiling_loss', 'temp_difference', 'installation_type_tracking', 'pressure', 'wind_cooling_effect', 'id', 'voltage', 'current', 'temperature', 'module_temperature', 'irradiance', 'wind_speed', 'panel_age', 'cloud_coverage', 'soiling_ratio', 'maintenance_count', 'humidity']\n",
      "Categorical features: 3\n",
      "Numerical features: 9\n",
      "✓ All required pipeline components are available\n",
      "\n",
      "Loading test data...\n",
      "Test data loaded successfully: (12000, 16)\n",
      "Columns: ['id', 'temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure', 'string_id', 'error_code', 'installation_type']\n",
      "\n",
      "Starting prediction process...\n",
      "Found ID column: id\n",
      "================================================================================\n",
      "SOLAR PANEL PREDICTION PIPELINE\n",
      "================================================================================\n",
      "Input raw data shape: (12000, 16)\n",
      "\n",
      "Step 1: Fixing data types...\n",
      "\n",
      "=== FIXING DATA TYPES FOR PREDICTION DATA ===\n",
      "\n",
      "Processing humidity:\n",
      "Original dtype: object\n",
      "Non-numeric values found in humidity:\n",
      "humidity\n",
      "error      35\n",
      "badval     23\n",
      "unknown    15\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 73\n",
      "Valid numeric values: 11927\n",
      "Min: 0.010\n",
      "Max: 99.982\n",
      "Mean: 49.822\n",
      "\n",
      "Processing wind_speed:\n",
      "Original dtype: object\n",
      "Non-numeric values found in wind_speed:\n",
      "wind_speed\n",
      "unknown    30\n",
      "error      28\n",
      "badval     23\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 82\n",
      "Valid numeric values: 11918\n",
      "Min: 0.001\n",
      "Max: 14.999\n",
      "Mean: 7.477\n",
      "\n",
      "Processing pressure:\n",
      "Original dtype: object\n",
      "Non-numeric values found in pressure:\n",
      "pressure\n",
      "badval     24\n",
      "unknown    22\n",
      "error      19\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 65\n",
      "Valid numeric values: 11935\n",
      "Min: 974.967\n",
      "Max: 1053.679\n",
      "Mean: 1013.122\n",
      "\n",
      "Step 2: Applying imputation pipeline...\n",
      "\n",
      "=== APPLYING IMPUTATION PIPELINE ===\n",
      "Data shape before imputation: (12000, 16)\n",
      "Missing values before imputation:\n",
      "temperature            582\n",
      "irradiance             615\n",
      "humidity                73\n",
      "panel_age              607\n",
      "maintenance_count      609\n",
      "soiling_ratio          610\n",
      "voltage                547\n",
      "current                587\n",
      "module_temperature     580\n",
      "cloud_coverage         582\n",
      "wind_speed              82\n",
      "pressure                65\n",
      "error_code            3611\n",
      "installation_type     2979\n",
      "dtype: int64\n",
      "Data shape after imputation: (12000, 16)\n",
      "Remaining missing values after imputation: 0\n",
      "\n",
      "Step 3: Applying feature engineering...\n",
      "\n",
      "=== APPLYING FEATURE ENGINEERING ===\n",
      "Data shape before feature engineering: (12000, 16)\n",
      "Data shape after feature engineering: (12000, 30)\n",
      "\n",
      "Step 4: Dropping specified features...\n",
      "\n",
      "=== DROPPING FEATURES ===\n",
      "Features to drop: ['soiling_loss', 'temp_difference', 'installation_type_tracking', 'pressure', 'wind_cooling_effect', 'id', 'voltage', 'current', 'temperature', 'module_temperature', 'irradiance', 'wind_speed', 'panel_age', 'cloud_coverage', 'soiling_ratio', 'maintenance_count', 'humidity']\n",
      "Dropped features: ['soiling_loss', 'temp_difference', 'installation_type_tracking', 'pressure', 'wind_cooling_effect', 'id', 'voltage', 'current', 'temperature', 'module_temperature', 'irradiance', 'wind_speed', 'panel_age', 'cloud_coverage', 'soiling_ratio', 'maintenance_count', 'humidity']\n",
      "Data shape after dropping features: (12000, 13)\n",
      "\n",
      "Step 5: Preprocessing features...\n",
      "\n",
      "=== PREPROCESSING FEATURES ===\n",
      "Selected feature columns: 13 features\n",
      "✓ Encoded categorical feature: string_id\n",
      "✓ Encoded categorical feature: error_code\n",
      "✓ Encoded categorical feature: installation_type\n",
      "✓ Applied numerical preprocessing\n",
      "Final preprocessed data shape: (12000, 12)\n",
      "\n",
      "Step 6: Making predictions...\n",
      "Step 7: Transforming predictions to original scale...\n",
      "✓ Generated 12000 predictions\n",
      "Prediction statistics:\n",
      "  Min: 0.2255\n",
      "  Max: 0.8967\n",
      "  Mean: 0.5179\n",
      "  Std: 0.0972\n",
      "\n",
      "✓ Predictions saved to: solar_efficiency_predictions.csv\n",
      "Output format: ['id', 'efficiency']\n",
      "\n",
      "================================================================================\n",
      "PREDICTION PIPELINE COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "Generated predictions for 12000 samples\n",
      "Results saved to: solar_efficiency_predictions.csv\n",
      "\n",
      "Sample predictions:\n",
      "   id  efficiency\n",
      "0   0    0.380456\n",
      "1   1    0.550307\n",
      "2   2    0.525679\n",
      "3   3    0.458207\n",
      "4   4    0.483849\n",
      "5   5    0.670635\n",
      "6   6    0.613482\n",
      "7   7    0.467650\n",
      "8   8    0.457755\n",
      "9   9    0.433987\n",
      "\n",
      "Pipeline executed successfully!\n",
      "Total predictions generated: 12000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras imports for the ANNRegressor\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.regularizers import l1_l2\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: TensorFlow not available. ANNRegressor will not work.\")\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Include the exact ANNRegressor class from training\n",
    "class ANNRegressor:\n",
    "    \"\"\"\n",
    "    Custom ANN Regressor wrapper that mimics scikit-learn interface\n",
    "    This is the exact class definition used during model training.\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons=128, layers=3, dropout_rate=0.3, \n",
    "                 learning_rate=0.001, l1_reg=0.0, l2_reg=0.01,\n",
    "                 epochs=200, batch_size=32, validation_split=0.2,\n",
    "                 patience=20, verbose=0):\n",
    "        self.neurons = neurons\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "        self.history_ = None\n",
    "        \n",
    "    def _build_model(self, input_dim):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        if not TENSORFLOW_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow is required for ANNRegressor but is not installed.\")\n",
    "            \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(self.neurons, \n",
    "                       input_dim=input_dim,\n",
    "                       activation='relu',\n",
    "                       kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.layers - 1):\n",
    "            layer_neurons = max(self.neurons // (2 ** i), 32)\n",
    "            model.add(Dense(layer_neurons,\n",
    "                           activation='relu',\n",
    "                           kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the neural network\"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        if len(y.shape) > 1:\n",
    "            y = y.flatten()\n",
    "        \n",
    "        self.model_ = self._build_model(X.shape[1])\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=self.patience, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(patience=self.patience//2, factor=0.5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        self.history_ = self.model_.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        predictions = self.model_.predict(X, verbose=0)\n",
    "        return predictions.flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'neurons': self.neurons,\n",
    "            'layers': self.layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'l1_reg': self.l1_reg,\n",
    "            'l2_reg': self.l2_reg,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'validation_split': self.validation_split,\n",
    "            'patience': self.patience,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "class SolarPanelPredictionPipeline:\n",
    "    def __init__(self, model_path='model/best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize prediction pipeline with trained model\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model_package = None\n",
    "        self.load_trained_model()\n",
    "    \n",
    "    def load_trained_model(self):\n",
    "        \"\"\"\n",
    "        Load the trained model and all preprocessing components\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                self.model_package = pickle.load(f)\n",
    "            \n",
    "            # Load all components from the training pipeline\n",
    "            self.best_model = self.model_package['model']\n",
    "            self.preprocessor = self.model_package['preprocessor']\n",
    "            self.label_encoders = self.model_package['label_encoders']\n",
    "            self.target_transformer = self.model_package['target_transformer']\n",
    "            self.imputer = self.model_package.get('imputer', None)\n",
    "            self.feature_cols = self.model_package['feature_names']\n",
    "            self.categorical_cols = self.model_package['categorical_cols']\n",
    "            self.numerical_cols = self.model_package['numerical_cols']\n",
    "            self.best_model_name = self.model_package.get('best_model_name', 'Unknown')\n",
    "            self.best_score = self.model_package.get('best_score', 0)\n",
    "            self.features_to_drop = self.model_package.get('features_to_drop', [])  # NEW: Load dropped features\n",
    "            \n",
    "            print(f\"Model loaded successfully: {self.best_model_name}\")\n",
    "            print(f\"Model score: {self.best_score:.4f}\")\n",
    "            print(f\"Expected features: {len(self.feature_cols)}\")\n",
    "            print(f\"Features to drop: {self.features_to_drop}\")\n",
    "            print(f\"Categorical features: {len(self.categorical_cols)}\")\n",
    "            print(f\"Numerical features: {len(self.numerical_cols)}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.model_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading model: {str(e)}\")\n",
    "    \n",
    "    def fix_data_types(self, df, dataset_name=\"PREDICTION\"):\n",
    "        \"\"\"\n",
    "        Fix data type inconsistencies for specific columns\n",
    "        This matches the exact logic from the training pipeline\n",
    "        \"\"\"\n",
    "        df_fixed = df.copy()\n",
    "        \n",
    "        # Define columns that should be numeric (same as training)\n",
    "        numeric_columns_to_fix = ['humidity', 'wind_speed', 'pressure']\n",
    "        \n",
    "        print(f\"\\n=== FIXING DATA TYPES FOR {dataset_name} ===\")\n",
    "        \n",
    "        for col in numeric_columns_to_fix:\n",
    "            if col in df_fixed.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                print(f\"Original dtype: {df_fixed[col].dtype}\")\n",
    "                \n",
    "                if df_fixed[col].dtype == 'object':\n",
    "                    try:\n",
    "                        numeric_conversion = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        non_numeric_mask = pd.isna(numeric_conversion) & df_fixed[col].notna()\n",
    "                        \n",
    "                        if non_numeric_mask.any():\n",
    "                            print(f\"Non-numeric values found in {col}:\")\n",
    "                            non_numeric_values = df_fixed.loc[non_numeric_mask, col].value_counts()\n",
    "                            print(non_numeric_values.head(10))\n",
    "                            \n",
    "                            df_fixed[col] = df_fixed[col].astype(str)\n",
    "                            df_fixed[col] = df_fixed[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "                            df_fixed[col] = df_fixed[col].str.strip()\n",
    "                            df_fixed[col] = df_fixed[col].replace('', np.nan)\n",
    "                            df_fixed[col] = df_fixed[col].replace('nan', np.nan)\n",
    "                            \n",
    "                        df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        \n",
    "                        print(f\"Converted dtype: {df_fixed[col].dtype}\")\n",
    "                        print(f\"Missing values after conversion: {df_fixed[col].isnull().sum()}\")\n",
    "                        print(f\"Valid numeric values: {df_fixed[col].notna().sum()}\")\n",
    "                        \n",
    "                        if df_fixed[col].notna().any():\n",
    "                            print(f\"Min: {df_fixed[col].min():.3f}\")\n",
    "                            print(f\"Max: {df_fixed[col].max():.3f}\")\n",
    "                            print(f\"Mean: {df_fixed[col].mean():.3f}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting {col}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"{col} is already numeric type: {df_fixed[col].dtype}\")\n",
    "        \n",
    "        return df_fixed\n",
    "    \n",
    "    def apply_imputation(self, df):\n",
    "        \"\"\"\n",
    "        Apply the fitted imputation pipeline to the raw data\n",
    "        \"\"\"\n",
    "        if self.imputer is None:\n",
    "            print(\"Warning: No imputation pipeline found. Proceeding without imputation.\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"\\n=== APPLYING IMPUTATION PIPELINE ===\")\n",
    "        print(f\"Data shape before imputation: {df.shape}\")\n",
    "        print(f\"Missing values before imputation:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
    "        \n",
    "        df_imputed = self.imputer.transform(df)\n",
    "        \n",
    "        print(f\"Data shape after imputation: {df_imputed.shape}\")\n",
    "        remaining_missing = df_imputed.isnull().sum().sum()\n",
    "        print(f\"Remaining missing values after imputation: {remaining_missing}\")\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def apply_feature_engineering(self, df):\n",
    "        \"\"\"\n",
    "        Apply the same feature engineering as used during training\n",
    "        NEW: This step was missing in the original prediction code\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== APPLYING FEATURE ENGINEERING ===\")\n",
    "        print(f\"Data shape before feature engineering: {df.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Import the same feature engineering class used in training\n",
    "            from utils.feature_engineering import SolarFeatureEngineering\n",
    "            feature_engineer = SolarFeatureEngineering()\n",
    "            df_engineered = feature_engineer.create_solar_features(df)\n",
    "            \n",
    "            print(f\"Data shape after feature engineering: {df_engineered.shape}\")\n",
    "            \n",
    "            # Verify no missing values in new features\n",
    "            new_missing = df_engineered.isnull().sum().sum()\n",
    "            if new_missing > 0:\n",
    "                print(f\"Warning: {new_missing} missing values found after feature engineering\")\n",
    "                print(\"Missing values by column:\")\n",
    "                print(df_engineered.isnull().sum()[df_engineered.isnull().sum() > 0])\n",
    "            \n",
    "            return df_engineered\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Warning: feature_engineering module not found. Skipping feature engineering.\")\n",
    "            print(\"This may cause prediction errors if the model expects engineered features.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature engineering: {str(e)}\")\n",
    "            print(\"Proceeding without feature engineering.\")\n",
    "            return df\n",
    "    \n",
    "    def drop_features(self, df):\n",
    "        \"\"\"\n",
    "        Drop the same features that were dropped during training\n",
    "        NEW: This step was missing in the original prediction code\n",
    "        \"\"\"\n",
    "        if not self.features_to_drop:\n",
    "            print(\"No features to drop.\")\n",
    "            return df\n",
    "            \n",
    "        print(f\"\\n=== DROPPING FEATURES ===\")\n",
    "        print(f\"Features to drop: {self.features_to_drop}\")\n",
    "        \n",
    "        available_features = [col for col in self.features_to_drop if col in df.columns]\n",
    "        unavailable_features = [col for col in self.features_to_drop if col not in df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            df_dropped = df.drop(columns=available_features)\n",
    "            print(f\"Dropped features: {available_features}\")\n",
    "            print(f\"Data shape after dropping features: {df_dropped.shape}\")\n",
    "        else:\n",
    "            df_dropped = df\n",
    "            print(\"No features were dropped (none found in dataset)\")\n",
    "        \n",
    "        if unavailable_features:\n",
    "            print(f\"Warning: Features not found in dataset: {unavailable_features}\")\n",
    "        \n",
    "        return df_dropped\n",
    "    \n",
    "    def preprocess_features(self, df):\n",
    "        \"\"\"\n",
    "        Apply the same preprocessing as used during training\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== PREPROCESSING FEATURES ===\")\n",
    "        \n",
    "        # Step 1: Select only the features that were used during training\n",
    "        missing_features = [col for col in self.feature_cols if col not in df.columns]\n",
    "        if missing_features:\n",
    "            print(f\"Error: Missing required features: {missing_features}\")\n",
    "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "        \n",
    "        X = df[self.feature_cols].copy()\n",
    "        print(f\"Selected feature columns: {X.shape[1]} features\")\n",
    "        \n",
    "        # Step 2: Handle categorical encoding using fitted label encoders\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X.columns:\n",
    "                le = self.label_encoders[col]\n",
    "                \n",
    "                # Handle unseen categories\n",
    "                unique_train_categories = set(le.classes_)\n",
    "                unique_test_categories = set(X[col].astype(str).unique())\n",
    "                unseen_categories = unique_test_categories - unique_train_categories\n",
    "                \n",
    "                if unseen_categories:\n",
    "                    print(f\"Warning: Unseen categories in {col}: {unseen_categories}\")\n",
    "                    most_frequent_category = le.classes_[0]\n",
    "                    X[col] = X[col].astype(str).replace(list(unseen_categories), most_frequent_category)\n",
    "                \n",
    "                X[col] = le.transform(X[col].astype(str))\n",
    "                print(f\"✓ Encoded categorical feature: {col}\")\n",
    "        \n",
    "        # Step 3: Apply numerical preprocessing using fitted preprocessor\n",
    "        X_scaled = self.preprocessor.transform(X)\n",
    "        \n",
    "        # Step 4: Convert back to DataFrame for consistency\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)\n",
    "        \n",
    "        print(f\"✓ Applied numerical preprocessing\")\n",
    "        print(f\"Final preprocessed data shape: {X_scaled.shape}\")\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_pred):\n",
    "        \"\"\"\n",
    "        Apply inverse transformation to predictions to get them back to original scale\n",
    "        \"\"\"\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_pred_original = self.target_transformer.inverse_transform(y_pred_reshaped).flatten()\n",
    "        return y_pred_original\n",
    "    \n",
    "    def predict(self, raw_data):\n",
    "        \"\"\"\n",
    "        Complete prediction pipeline that mirrors the training pipeline exactly\n",
    "        NOW INCLUDES: Feature Engineering and Feature Dropping steps\n",
    "        \n",
    "        Pipeline: Raw Data → Fix Data Types → Imputation → Feature Engineering → Drop Features → Preprocessing → Prediction\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"SOLAR PANEL PREDICTION PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Input raw data shape: {raw_data.shape}\")\n",
    "        \n",
    "        # Step 1: Fix data types (same as training)\n",
    "        print(\"\\nStep 1: Fixing data types...\")\n",
    "        df_fixed = self.fix_data_types(raw_data, \"PREDICTION DATA\")\n",
    "        \n",
    "        # Step 2: Apply imputation pipeline\n",
    "        print(\"\\nStep 2: Applying imputation pipeline...\")\n",
    "        df_imputed = self.apply_imputation(df_fixed)\n",
    "        \n",
    "        # Step 3: Apply feature engineering (NEW STEP)\n",
    "        print(\"\\nStep 3: Applying feature engineering...\")\n",
    "        df_engineered = self.apply_feature_engineering(df_imputed)\n",
    "        \n",
    "        # Step 4: Drop features (NEW STEP)\n",
    "        print(\"\\nStep 4: Dropping specified features...\")\n",
    "        df_final = self.drop_features(df_engineered)\n",
    "        \n",
    "        # Step 5: Preprocess features (categorical encoding + numerical scaling)\n",
    "        print(\"\\nStep 5: Preprocessing features...\")\n",
    "        X_processed = self.preprocess_features(df_final)\n",
    "        \n",
    "        # Step 6: Make predictions on transformed scale\n",
    "        print(f\"\\nStep 6: Making predictions...\")\n",
    "        y_pred_transformed = self.best_model.predict(X_processed)\n",
    "        \n",
    "        # Step 7: Transform predictions back to original scale\n",
    "        print(f\"Step 7: Transforming predictions to original scale...\")\n",
    "        y_pred_original = self.inverse_transform_predictions(y_pred_transformed)\n",
    "        \n",
    "        print(f\"✓ Generated {len(y_pred_original)} predictions\")\n",
    "        print(f\"Prediction statistics:\")\n",
    "        print(f\"  Min: {y_pred_original.min():.4f}\")\n",
    "        print(f\"  Max: {y_pred_original.max():.4f}\")\n",
    "        print(f\"  Mean: {y_pred_original.mean():.4f}\")\n",
    "        print(f\"  Std: {y_pred_original.std():.4f}\")\n",
    "        \n",
    "        return y_pred_original\n",
    "    \n",
    "    def predict_with_id(self, raw_data, id_column='id'):\n",
    "        \"\"\"\n",
    "        Make predictions and return with original IDs\n",
    "        \"\"\"\n",
    "        if id_column in raw_data.columns:\n",
    "            ids = raw_data[id_column].copy()\n",
    "            print(f\"Found ID column: {id_column}\")\n",
    "        else:\n",
    "            ids = range(len(raw_data))\n",
    "            print(f\"No ID column found, using sequential IDs\")\n",
    "        \n",
    "        predictions = self.predict(raw_data)\n",
    "        \n",
    "        results = pd.DataFrame({\n",
    "            id_column: ids,\n",
    "            'efficiency': predictions\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_predictions(self, raw_data, output_path='predictions.csv', id_column='id'):\n",
    "        \"\"\"\n",
    "        Generate predictions and save to CSV\n",
    "        \"\"\"\n",
    "        results = self.predict_with_id(raw_data, id_column)\n",
    "        results.to_csv(output_path, index=False)\n",
    "        print(f\"\\n✓ Predictions saved to: {output_path}\")\n",
    "        print(f\"Output format: {list(results.columns)}\")\n",
    "        return results\n",
    "    \n",
    "    def validate_pipeline_compatibility(self):\n",
    "        \"\"\"\n",
    "        Validate that the loaded model has all required components\n",
    "        \"\"\"\n",
    "        required_components = [\n",
    "            'model', 'preprocessor', 'label_encoders', \n",
    "            'target_transformer', 'feature_names'\n",
    "        ]\n",
    "        \n",
    "        optional_components = ['imputer', 'features_to_drop']\n",
    "        \n",
    "        missing_components = []\n",
    "        for component in required_components:\n",
    "            if component not in self.model_package:\n",
    "                missing_components.append(component)\n",
    "        \n",
    "        if missing_components:\n",
    "            print(f\"Error: Missing required components in saved model: {missing_components}\")\n",
    "            return False\n",
    "        \n",
    "        missing_optional = []\n",
    "        for component in optional_components:\n",
    "            if component not in self.model_package:\n",
    "                missing_optional.append(component)\n",
    "        \n",
    "        if missing_optional:\n",
    "            print(f\"Warning: Missing optional components: {missing_optional}\")\n",
    "            print(\"Pipeline will continue but some features may not be available.\")\n",
    "        \n",
    "        print(\"✓ All required pipeline components are available\")\n",
    "        return True\n",
    "\n",
    "# Utility functions\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"Load test data from CSV\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Test data loaded successfully: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading test data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize prediction pipeline\n",
    "        print(\"Initializing Solar Panel Prediction Pipeline...\")\n",
    "        pipeline = SolarPanelPredictionPipeline(model_path='model/test_model.pkl')\n",
    "        \n",
    "        # Validate pipeline compatibility\n",
    "        if not pipeline.validate_pipeline_compatibility():\n",
    "            print(\"Warning: Pipeline compatibility issues detected. Proceeding anyway...\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"\\nLoading test data...\")\n",
    "        test_data = load_test_data('dataset/test.csv')\n",
    "        \n",
    "        # Generate and save predictions\n",
    "        print(f\"\\nStarting prediction process...\")\n",
    "        predictions = pipeline.save_predictions(\n",
    "            raw_data=test_data,\n",
    "            output_path='solar_efficiency_predictions.csv',\n",
    "            id_column='id'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREDICTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Generated predictions for {len(predictions)} samples\")\n",
    "        print(f\"Results saved to: solar_efficiency_predictions.csv\")\n",
    "        \n",
    "        # Display sample predictions\n",
    "        print(f\"\\nSample predictions:\")\n",
    "        print(predictions.head(10))\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main prediction pipeline\n",
    "    try:\n",
    "        predictions = main()\n",
    "        print(f\"\\nPipeline executed successfully!\")\n",
    "        print(f\"Total predictions generated: {len(predictions)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
